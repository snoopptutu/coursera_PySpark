{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": "from IPython.display import Markdown, display\ndef printmd(string):\n    display(Markdown('# <span style=\"color:red\">'+string+'</span>'))\n\n\nif ('sc' in locals() or 'sc' in globals()):\n    printmd('<<<<<!!!!! It seems that you are running in a IBM Watson Studio Apache Spark Notebook. Please run it in an IBM Watson Studio Default Runtime (without Apache Spark) !!!!!>>>>>')"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Collecting pyspark==2.4.5\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/5a/271c416c1c2185b6cb0151b29a91fff6fcaed80173c8584ff6d20e46b465/pyspark-2.4.5.tar.gz (217.8MB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 217.8MB 140kB/s  eta 0:00:01    |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                | 103.9MB 4.2MB/s eta 0:00:27\n\u001b[?25hCollecting py4j==0.10.7 (from pyspark==2.4.5)\n\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n\u001b[K     |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 204kB 47.8MB/s eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: pyspark\n  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/bf/db/04/61d66a5939364e756eb1c1be4ec5bdce6e04047fc7929a3c3c\nSuccessfully built pyspark\nInstalling collected packages: py4j, pyspark\nSuccessfully installed py4j-0.10.7 pyspark-2.4.5\n"
                }
            ],
            "source": "!pip install pyspark==2.4.5"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": "try:\n    from pyspark import SparkContext, SparkConf\n    from pyspark.sql import SparkSession\nexcept ImportError as e:\n    printmd('<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>')"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n\nspark = SparkSession \\\n    .builder \\\n    .getOrCreate()"
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "[368, 369, 626, 824, 460, 994, 632, 179, 154, 486]\n"
                }
            ],
            "source": "import random\nrdd = sc.parallelize(random.sample(range(1000),100))\nprint(rdd.take(10))"
        },
        {
            "cell_type": "code",
            "execution_count": 22,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "490.67\n"
                }
            ],
            "source": "# mean\nsum = rdd.sum()\nn = rdd.count()\nmean = sum/float(n)\nprint(mean)"
        },
        {
            "cell_type": "code",
            "execution_count": 23,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "497.5\n"
                }
            ],
            "source": "# median\nsorted = rdd.sortBy(lambda x: x)\nsorted_and_index = sorted.zipWithIndex()\nsorted_with_index = sorted_and_index.map(lambda x: (x[1],x[0]))\nn = sorted_with_index.count()\n\nif n % 2 == 1:\n    index = (n-1)/2\n    median = sorted_with_index.lookup(index)[0]\nelse:\n    index1 = n/2 - 1\n    index2 = n/2\n    median = (sorted_with_index.lookup(index1)[0] +  sorted_with_index.lookup(index2)[0]) / 2\n\nprint(median)"
        },
        {
            "cell_type": "code",
            "execution_count": 24,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "261.41117248503366\n"
                }
            ],
            "source": "# standard deviation\nfrom math import sqrt\nsd = sqrt(rdd.map(lambda x: pow(x-mean,2)).sum()/n)\nprint(sd)"
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "-0.04931414014401959"
                    },
                    "execution_count": 25,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# skewness\nn = float(n)\nskewness = (rdd.map(lambda x: pow(x-mean,3)).sum()/pow(sd,3))/n\nskewness"
        },
        {
            "cell_type": "code",
            "execution_count": 26,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "2.059206389310504"
                    },
                    "execution_count": 26,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# kurtosis\nkurtosis = (rdd.map(lambda x: pow(x-mean,4)).sum()/pow(sd,4))/n\nkurtosis"
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "-4486.401000000001\n-0.053319506006586595\n"
                }
            ],
            "source": "# covariance\nrddX = sc.parallelize(random.sample(range(1000),100))\nrddY = sc.parallelize(random.sample(range(1000),100))\nmeanX = rddX.sum()/float(rddX.count())\nmeanY = rddY.sum()/float(rddY.count())\nrddXY = rddX.zip(rddY)\ncovXY = rddXY.map(lambda x: (x[0] - meanX)*(x[1] - meanY)).sum() / rddXY.count()\n\n# correlation\nsdX = sqrt(rddX.map(lambda x: pow(x-meanX,2)).sum()/rddX.count())\nsdY = sqrt(rddY.map(lambda x: pow(x-meanY,2)).sum()/rddY.count())\ncorrXY = covXY/(sdX*sdY)\n\nprint(covXY)\nprint(corrXY)"
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "array([[ 1.        , -0.17785432, -0.04262465, -0.05200905],\n       [-0.17785432,  1.        ,  0.10953086, -0.03486309],\n       [-0.04262465,  0.10953086,  1.        ,  0.15173876],\n       [-0.05200905, -0.03486309,  0.15173876,  1.        ]])"
                    },
                    "execution_count": 28,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# covariance matrix\nfrom pyspark.mllib.stat import Statistics\ncol1 = sc.parallelize(random.sample(range(1000),100))\ncol2 = sc.parallelize(random.sample(range(1000),100))\ncol3 = sc.parallelize(random.sample(range(1000),100))\ncol4 = sc.parallelize(random.sample(range(1000),100))\ndata = col1.zip(col2).zip(col3).zip(col4)\ndata = data.map(lambda x: [x[0][0][0],x[0][0][1],x[0][1],x[1]])\nStatistics.corr(data)"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}