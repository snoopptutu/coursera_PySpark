{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": "from IPython.display import Markdown, display\ndef printmd(string):\n    display(Markdown('# <span style=\"color:red\">'+string+'</span>'))\n\n\nif ('sc' in locals() or 'sc' in globals()):\n    printmd('<<<<<!!!!! It seems that you are running in a IBM Watson Studio Apache Spark Notebook. Please run it in an IBM Watson Studio Default Runtime (without Apache Spark) !!!!!>>>>>')"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "Requirement already satisfied: pyspark==2.4.5 in /opt/conda/envs/Python36/lib/python3.6/site-packages (2.4.5)\nRequirement already satisfied: py4j==0.10.7 in /opt/conda/envs/Python36/lib/python3.6/site-packages (from pyspark==2.4.5) (0.10.7)\n"
                }
            ],
            "source": "!pip install pyspark==2.4.5"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [],
            "source": "try:\n    from pyspark import SparkContext, SparkConf\n    from pyspark.sql import SparkSession\nexcept ImportError as e:\n    printmd('<<<<<!!!!! Please restart your kernel after installing Apache Spark !!!!!>>>>>')"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [],
            "source": "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n\nspark = SparkSession \\\n    .builder \\\n    .getOrCreate()"
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "[136, 541, 403, 437, 145, 499, 339, 473, 504, 167]\n"
                }
            ],
            "source": "import random\nrdd = sc.parallelize(random.sample(range(1000),100))\nprint(rdd.take(10))"
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "509.12\n"
                }
            ],
            "source": "# mean\nsum = rdd.sum()\nn = rdd.count()\nmean = sum/float(n)\nprint(mean)"
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "487.5\n"
                }
            ],
            "source": "# median\nsorted = rdd.sortBy(lambda x: x)\nsorted_and_index = sorted.zipWithIndex()\nsorted_with_index = sorted_and_index.map(lambda x: (x[1],x[0]))\nn = sorted_with_index.count()\n\nif n % 2 == 1:\n    index = (n-1)/2\n    median = sorted_with_index.lookup(index)[0]\nelse:\n    index1 = n/2 - 1\n    index2 = n/2\n    median = (sorted_with_index.lookup(index1)[0] +  sorted_with_index.lookup(index2)[0]) / 2\n\nprint(median)"
        },
        {
            "cell_type": "code",
            "execution_count": 39,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "288.0346888831274\n"
                }
            ],
            "source": "# standard deviation\nfrom math import sqrt\nsd = sqrt(rdd.map(lambda x: pow(x-mean,2)).sum()/n)\nprint(sd)"
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "-0.23349202874223912"
                    },
                    "execution_count": 41,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# skewness\nn = float(n)\nskewness = (rdd.map(lambda x: pow(x-mean,3)).sum()/pow(stdv,3))/n\nskewness"
        },
        {
            "cell_type": "code",
            "execution_count": 43,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "1.85327962407774"
                    },
                    "execution_count": 43,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# kurtosis\nkurtosis = (rdd.map(lambda x: pow(x-mean,4)).sum()/pow(stdv,4))/n\nkurtosis"
        },
        {
            "cell_type": "code",
            "execution_count": 48,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "3663.377599999997\n0.047385147192775055\n"
                }
            ],
            "source": "# covariance\nrddX = sc.parallelize(random.sample(range(1000),100))\nrddY = sc.parallelize(random.sample(range(1000),100))\nmeanX = rddX.sum()/float(rddX.count())\nmeanY = rddY.sum()/float(rddY.count())\nrddXY = rddX.zip(rddY)\ncovXY = rddXY.map(lambda x: (x[0] - meanX)*(x[1] - meanY)).sum() / rddXY.count()\n\n# correlation\nsdX = sqrt(rddX.map(lambda x: pow(x-meanX,2)).sum()/rddX.count())\nsdY = sqrt(rddY.map(lambda x: pow(x-meanY,2)).sum()/rddY.count())\ncorrXY = covXY/(sdX*sdY)\n\nprint(covXY)\nprint(corrXY)"
        },
        {
            "cell_type": "code",
            "execution_count": 55,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "array([[ 1.        ,  0.11566108, -0.22179425,  0.03930211],\n       [ 0.11566108,  1.        , -0.0432733 ,  0.02781347],\n       [-0.22179425, -0.0432733 ,  1.        ,  0.08601731],\n       [ 0.03930211,  0.02781347,  0.08601731,  1.        ]])"
                    },
                    "execution_count": 55,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "# covariance matrix\nfrom pyspark.mllib.stat import Statistics\ncol1 = sc.parallelize(random.sample(range(1000),100))\ncol2 = sc.parallelize(random.sample(range(1000),100))\ncol3 = sc.parallelize(random.sample(range(1000),100))\ncol4 = sc.parallelize(random.sample(range(1000),100))\ndata = col1.zip(col2).zip(col3).zip(col4)\ndata = data.map(lambda x: [x[0][0][0],x[0][0][1],x[0][1],x[1]])\nStatistics.corr(data)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}